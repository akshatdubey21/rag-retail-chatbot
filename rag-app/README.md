# ðŸ“„ RAG Chat Bot â€” Chat with Retail PDFs

A production-ready **Retrieval-Augmented Generation (RAG)** web application that lets users upload retail PDF documents and ask natural-language questions about them. Answers are grounded strictly in the document content.

---

## Architecture

```
User uploads PDF
      â”‚
      â–¼
  Extract text (PyPDF)
      â”‚
      â–¼
  Split into chunks (LangChain RecursiveCharacterTextSplitter)
      â”‚
      â–¼
  Generate embeddings (sentence-transformers/all-MiniLM-L6-v2)
      â”‚
      â–¼
  Store in FAISS vector index (local disk)
      â”‚
      â–¼
  User sends question
      â”‚
      â–¼
  Retrieve top-k relevant chunks (FAISS similarity search)
      â”‚
      â–¼
  Build prompt with context + question
      â”‚
      â–¼
  Generate answer (google/flan-t5-base via HuggingFace transformers)
      â”‚
      â–¼
  Return answer + source references to frontend
```

---

## Tech Stack

| Layer     | Technology                                          |
| --------- | --------------------------------------------------- |
| Frontend  | React 18, Vite, Tailwind CSS, Axios                 |
| Backend   | Python 3.10+, FastAPI, LangChain                    |
| Vector DB | FAISS (local storage)                               |
| Embedding | HuggingFace `sentence-transformers/all-MiniLM-L6-v2`|
| LLM       | HuggingFace `google/flan-t5-base` (100 % free, local) |

---

## Project Structure

```
rag-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app & endpoints
â”‚   â”œâ”€â”€ rag_pipeline.py       # Orchestrates ingest â†’ retrieve â†’ generate
â”‚   â”œâ”€â”€ pdf_loader.py         # PDF text extraction & chunking
â”‚   â”œâ”€â”€ vector_store.py       # FAISS index management
â”‚   â”œâ”€â”€ llm.py                # HuggingFace LLM wrapper
â”‚   â”œâ”€â”€ config.py             # Settings from .env
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.jsx
â”‚       â”œâ”€â”€ App.jsx
â”‚       â”œâ”€â”€ services/api.js
â”‚       â”œâ”€â”€ styles/index.css
â”‚       â””â”€â”€ components/
â”‚           â”œâ”€â”€ Chat.jsx
â”‚           â”œâ”€â”€ Message.jsx
â”‚           â”œâ”€â”€ Upload.jsx
â”‚           â””â”€â”€ Sidebar.jsx
â”œâ”€â”€ data/                     # Created at runtime
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## Prerequisites

- **Python 3.10+** (with `pip`)
- **Node.js 18+** (with `npm`)
- **Git** (optional)

> âš ï¸ No paid API keys are required. Everything runs locally using free HuggingFace models.

---

## ðŸš€ Getting Started

### 1. Clone / open the project

```bash
cd rag-app
```

---

### 2. Backend Setup

```bash
# Navigate to backend
cd backend

# Create a virtual environment
python -m venv venv

# Activate it
# Windows PowerShell:
.\venv\Scripts\Activate.ps1
# macOS / Linux:
# source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start the server
python main.py
```

The API will be available at **http://localhost:8000**.  
Interactive docs at **http://localhost:8000/docs**.

> **First startup** will download the embedding model (~80 MB) and the LLM (~900 MB).  
> Subsequent starts are instant.

---

### 3. Frontend Setup

Open a **new terminal**:

```bash
# Navigate to frontend
cd frontend

# Install dependencies
npm install

# Start the dev server
npm run dev
```

Open **http://localhost:5173** in your browser.

---

## Usage

1. **Upload a PDF** â€” Click the "Upload PDF" button in the sidebar and select a retail PDF.
2. **Wait for processing** â€” The backend extracts text, chunks it, generates embeddings, and stores them in FAISS.
3. **Ask questions** â€” Type a question in the chat input and press Enter (or click the send button).
4. **View answers** â€” The AI responds with an answer derived strictly from the uploaded documents, along with source references (filename + page number).

---

## API Endpoints

| Method | Path      | Description                       |
| ------ | --------- | --------------------------------- |
| GET    | `/health` | Health check + uploaded file list |
| POST   | `/upload` | Upload a PDF (multipart/form-data)|
| POST   | `/chat`   | Send a question, get an answer    |

### Example â€” Upload

```bash
curl -X POST http://localhost:8000/upload \
  -F "file=@retail_report.pdf"
```

### Example â€” Chat

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "What were the total sales in Q3?"}'
```

---

## Environment Variables (`.env`)

| Variable         | Default                                      | Description                  |
| ---------------- | -------------------------------------------- | ---------------------------- |
| `HF_MODEL_NAME`  | `google/flan-t5-base`                        | HuggingFace generation model |
| `EMBEDDING_MODEL` | `sentence-transformers/all-MiniLM-L6-v2`    | Sentence-transformer model   |
| `CHUNK_SIZE`      | `500`                                        | Characters per text chunk    |
| `CHUNK_OVERLAP`   | `50`                                         | Overlap between chunks       |
| `FAISS_INDEX_PATH`| `data/faiss_index`                           | FAISS index directory        |
| `UPLOAD_DIR`      | `data/uploads`                               | Uploaded PDF storage         |
| `MAX_NEW_TOKENS`  | `512`                                        | Max tokens per LLM response  |

---

## Troubleshooting

| Issue | Fix |
|-------|-----|
| `torch` install fails | Use `pip install torch --index-url https://download.pytorch.org/whl/cpu` for CPU-only |
| CORS errors in browser | Make sure the backend is running on port 8000 |
| Slow first response | Models are downloaded on first run; subsequent starts are fast |
| Out of memory | Use `google/flan-t5-base` (default) instead of larger models |

---

## License

MIT
